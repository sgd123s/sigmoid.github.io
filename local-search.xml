<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>YOLO的数据增强与参数调整</title>
    <link href="/2024/07/16/YOLO%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E4%B8%8E%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/"/>
    <url>/2024/07/16/YOLO%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E4%B8%8E%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/</url>
    
    <content type="html"><![CDATA[<h2 id="YOLO的数据增强与参数调整"><a href="#YOLO的数据增强与参数调整" class="headerlink" title="YOLO的数据增强与参数调整"></a>YOLO的数据增强与参数调整</h2><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>​被某比赛的视觉任务反复折磨之后, 我发现 yolo 的泛化性能不仅在目标检测领域, 甚至在图像分类领域都有着很高的地位。</p><p>​事情是这样的, 我拿 resnet 的各个变种去训练 cifar100 , 训练出的模型泛化性能极其不行, 经常过拟合; 我拿cifar100 benchmark上面的sota们(State of the arts, 指的是在某一个领域做的Performance最好的model)去训练发现他们大部分是为了刷sota而做的模型, 他们模型的预测结果虽然是正确的但是置信度极低(0.1x), 拿训练好的模型去预测效果也很差。但是如果同样的输入放到 yolo 模型里面得到的输出正确率竟然高的惊人。</p><p>​说明一下前提, 我的输入仍然是训练集的东西, 但是由于实际环境的影响跟原来的训练集可能在饱和度、色调、亮度等方面都有不同。</p><p>​曾经用 cifar100 训练 resnet18&#x2F;34 的时候, 模型的过拟合现象严重, 我也曾经试着将预处理加上亮度、色调、饱和度的随机变化, 但是效果都很一般。</p><p>​因此,  我很好奇 YOLO 在数据增强阶段应用了什么样的操作, 另外, 在 yolo 的训练结果的 weight 中有 best_weights 和 last_weights 这两个权重, 我也想知道它评判每一轮的权重是否为 best 的依据是什么(？是不是只是简单的评价一下哪一轮的测试集正确率最高哪个就是best)</p><div class="note note-info">            <p>​2024.7.16: 再挖个坑, 等自己填, 感觉要等一段时间, 最近有点忙, 估计没工夫去看 yolo 源码, 或许可以直接GPT, 但是最好还是自己在源码里面找到这部分逻辑, 毕竟眼见为实。等填坑的时候一定要 callback 一下这里！</p>          </div>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>cv</tag>
      
      <tag>目标检测</tag>
      
      <tag>yolo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[DL]pytorch模型训练pipline总结</title>
    <link href="/2024/07/16/DL-pytorch%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83pipline%E6%80%BB%E7%BB%93/"/>
    <url>/2024/07/16/DL-pytorch%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83pipline%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h2 id="DL-pytorch模型训练pipline总结"><a href="#DL-pytorch模型训练pipline总结" class="headerlink" title="[DL]pytorch模型训练pipline总结"></a>[DL]pytorch模型训练pipline总结</h2><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>​在前面的<a href="https://sigmoidsee.github.io/2024/07/16/DL-%E5%9F%BA%E4%BA%8ELSTM%E7%9A%84%E6%96%87%E6%9C%AC%E6%83%85%E7%BB%AA%E8%AF%86%E5%88%AB%E5%AE%9E%E6%88%98/">LSTM实现文本情绪识别的博客</a>, 还有训练流程这一 part没写, 打算单独写个博客来总结一下 pytorch 模型训练的 pipline, 于是有了这一篇博客。</p><h3 id="训练pipline"><a href="#训练pipline" class="headerlink" title="训练pipline"></a>训练pipline</h3><p>​下面是一般训练模型的流程, 具体的任务可能有些细节上的差异, 但是总体的流程都是大差不差的。</p><h4 id="定义数据加载器"><a href="#定义数据加载器" class="headerlink" title="定义数据加载器"></a>定义数据加载器</h4><p>​这一步准备方式有多种, NLP与CV可能不尽相同, 可能有的NLP任务不需要transform预处理, 直接用<code>TensorDataset</code>来代替。具体任务具体分析, 但是总归要获得一个带着batch的DataLoader。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((<span class="hljs-number">0.5</span>,), (<span class="hljs-number">0.5</span>,))])<br>train_dataset = datasets.MNIST(root=<span class="hljs-string">&#x27;./data&#x27;</span>, train=<span class="hljs-literal">True</span>, download=<span class="hljs-literal">True</span>, transform=transform)<br>train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><h4 id="定义损失函数和优化器"><a href="#定义损失函数和优化器" class="headerlink" title="定义损失函数和优化器"></a>定义损失函数和优化器</h4><p>​需要说明的一点是不同的任务所需要的损失函数和优化器可能不尽相同, 在定义自己任务所需要的损失函数和优化器之前, 最好去网上做做功课然后定义, 可以尝试多种组合的损失函数和优化器来对比分析一下哪种组合的损失函数和优化器更好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># loss and optimization functions</span><br>lr=<span class="hljs-number">0.001</span><br>criterion = nn.BCELoss() <span class="hljs-comment"># 常用于二分类的损失函数</span><br>optimizer = torch.optim.Adam(net.parameters(), lr=lr)<br></code></pre></td></tr></table></figure><h4 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h4><p>​一般为了防止爆了显存, 一个 epoch 往往分成多个 batch 来计算, 这部分训练的过程往往放在一个 for 循环里面。</p><ul><li><p>定义 epoch</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">num_epochs = <span class="hljs-number">5</span><br></code></pre></td></tr></table></figure></li><li><p>开启两轮循环, 第一个 for 循环是为了训练完 <code>num_epochs</code>轮训练集, 第二个 for 循环是为了训练完一个 <code>epoch</code>里面的 <code>batches</code>, 每一个 epoch 循环完都要清空 loss。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">for</span> epoch in range(num_epochs):<br>    <span class="hljs-attribute">running_loss</span> = <span class="hljs-number">0</span>.<span class="hljs-number">0</span><br>    <span class="hljs-attribute">for</span> inputs, labels in train_loader:<br></code></pre></td></tr></table></figure></li><li><p>将输入和标签传送到GPU (如果可用)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">inputs, labels = inputs.to(device), labels.to(device)<br></code></pre></td></tr></table></figure></li><li><p>将优化器梯度清零</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">optimizer.zero_grad()<br></code></pre></td></tr></table></figure></li><li><p>前向传播</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">outputs = model(inputs)<br></code></pre></td></tr></table></figure></li><li><p>计算损失</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">loss = criterion(outputs, labels)<br></code></pre></td></tr></table></figure></li><li><p>反向传播</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">loss.backward()<br></code></pre></td></tr></table></figure></li><li><p>优化器进行参数更新</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">optimizer.step()<br></code></pre></td></tr></table></figure></li><li><p>累计损失</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">running_loss += loss.item()<br></code></pre></td></tr></table></figure></li><li><p>打印结果(二轮循环外)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Epoch [<span class="hljs-subst">&#123;epoch + <span class="hljs-number">1</span>&#125;</span>/<span class="hljs-subst">&#123;num_epochs&#125;</span>], Loss: <span class="hljs-subst">&#123;running_loss / <span class="hljs-built_in">len</span>(train_loader)&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure></li></ul><h3 id="评估模型-可选"><a href="#评估模型-可选" class="headerlink" title="评估模型(可选)"></a>评估模型(可选)</h3><p>​在实际过程中为了评估模型的泛化性能, 在训练完一轮训练数据或者循环完整个epochs之后, 往往都还会设置评估模型这一步, 下面说明一下评估模型的pipline。</p><ul><li><p>定义数据加载器(同上)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 准备测试数据集和数据加载器</span><br>test_dataset = datasets.MNIST(root=<span class="hljs-string">&#x27;./data&#x27;</span>, train=<span class="hljs-literal">False</span>, download=<span class="hljs-literal">True</span>, transform=transform)<br>test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure></li><li><p>切换模型到评估模式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.<span class="hljs-built_in">eval</span>()  <span class="hljs-comment"># 将模型设置为评估模式。评估模式下，某些层（如Dropout、BatchNorm）会有不同的行为。</span><br></code></pre></td></tr></table></figure></li><li><p>初始化变量以记录正确预测的数量和总样本数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">correct = <span class="hljs-number">0</span>  <span class="hljs-comment"># 用于存储正确预测的样本数量</span><br>total = <span class="hljs-number">0</span>  <span class="hljs-comment"># 用于存储测试样本的总数量</span><br></code></pre></td></tr></table></figure></li><li><p>开始评估</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 禁用梯度计算以加快评估速度</span><br><span class="hljs-keyword">with</span> torch.no_grad():  <span class="hljs-comment"># 禁用梯度计算，因为在评估过程中不需要反向传播</span><br>    <span class="hljs-keyword">for</span> inputs, labels <span class="hljs-keyword">in</span> test_loader:  <span class="hljs-comment"># 遍历测试数据集中的所有样本</span><br>        inputs, labels = inputs.to(device), labels.to(device)  <span class="hljs-comment"># 将数据移动到GPU（如果可用）</span><br>        outputs = model(inputs)  <span class="hljs-comment"># 获取模型的预测输出</span><br>        _, predicted = torch.<span class="hljs-built_in">max</span>(outputs.data, <span class="hljs-number">1</span>)  <span class="hljs-comment"># 获取每个输入样本的预测标签</span><br>        total += labels.size(<span class="hljs-number">0</span>)  <span class="hljs-comment"># 更新总样本数</span><br>        correct += (predicted == labels).<span class="hljs-built_in">sum</span>().item()  <span class="hljs-comment"># 更新正确预测的样本数量</span><br></code></pre></td></tr></table></figure></li><li><p>计算并打印模型在测试集上的准确率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">accuracy = <span class="hljs-number">100</span> * correct / total  <span class="hljs-comment"># 计算准确率</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Accuracy of the model on the test images: <span class="hljs-subst">&#123;accuracy&#125;</span> %&#x27;</span>)  <span class="hljs-comment"># 打印准确率</span><br></code></pre></td></tr></table></figure></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>​本文总结了 pytorch 模型训练的一般流程, 前提是假定你已经定义好自己的模型并且完成了实例化的。需要注意的是具体的任务场景下可能还需要额外的步骤, 但是大致都逃不了上面的这些步骤。</p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[DL]基于LSTM的文本情绪识别实战</title>
    <link href="/2024/07/16/DL-%E5%9F%BA%E4%BA%8ELSTM%E7%9A%84%E6%96%87%E6%9C%AC%E6%83%85%E7%BB%AA%E8%AF%86%E5%88%AB%E5%AE%9E%E6%88%98/"/>
    <url>/2024/07/16/DL-%E5%9F%BA%E4%BA%8ELSTM%E7%9A%84%E6%96%87%E6%9C%AC%E6%83%85%E7%BB%AA%E8%AF%86%E5%88%AB%E5%AE%9E%E6%88%98/</url>
    
    <content type="html"><![CDATA[<h2 id="基于LSTM的文本情绪识别实战"><a href="#基于LSTM的文本情绪识别实战" class="headerlink" title="基于LSTM的文本情绪识别实战"></a>基于LSTM的文本情绪识别实战</h2><p>​本项目基于LSTM实现, 应用的是 pytorch 深度学习框架。项目源码<a href="https://github.com/sigmoidsee/LSTMbyPytorch">这里</a>，因为我这是私密的库，如果将来有一天我的博客网站被你发现了，恰巧你又对这个项目感兴趣, 可以联系我给你放开权限hh.</p><h3 id="实现效果"><a href="#实现效果" class="headerlink" title="实现效果"></a>实现效果</h3><p>​能根据一段评价来预测这段评价是积极的还是消极的, 类似于二分器但又不能完全用传统的二分器来实现, 因为语言还有语序的因素。</p><p><img src="/images/post_4/1.png" alt="实现效果"></p><h3 id="实现过程"><a href="#实现过程" class="headerlink" title="实现过程"></a>实现过程</h3><p>​感觉大部分 NLP 任务的 pipline 都大差不差, 基本都要经历 数据清洗 -&gt; 数据集的划分 -&gt; 定义模型 -&gt; 实例模型 -&gt; 模型训练 -&gt; 模型评估 这几部分, 关于word2vec,这次任务是集成在定义模型里了, 我了解还不是很多, 目前只知道 one-hot 这种最简单的编码方式, 后面了解多了会专门写一篇博客来专门聊这个问题。</p><h4 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h4><p>​训练数据集的清洗主要包括去除训练数据中的特殊符号(!”#$%&amp;’()*+,-.&#x2F;:;&lt;&#x3D;&gt;?@[]^_&#96;{|}~)、去除换行符、去除空格、word2int映射、剪枝与填充、标签数据的清洗这几个操作。</p><ul><li><p>去除训练数据中的特殊符号</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> string <span class="hljs-keyword">import</span> punctuation<br><br><span class="hljs-built_in">print</span>(punctuation)<br><br><span class="hljs-comment"># get rid of punctuation</span><br>reviews = reviews.lower() <span class="hljs-comment"># lowercase, standardize</span><br>all_text = <span class="hljs-string">&#x27;&#x27;</span>.join([c <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> reviews <span class="hljs-keyword">if</span> c <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> punctuation])<br></code></pre></td></tr></table></figure></li><li><p>去除训练数据中的换行符</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">这里的数据清洗一定要核对一下长度,有时候明明文本文件没有换行符,但是读取是在最后多了一个换行符</span><br><span class="hljs-string">导致长度+1</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>reviews_split = all_text.split(<span class="hljs-string">&#x27;\n&#x27;</span>)<br>reviews_split = [review <span class="hljs-keyword">for</span> review <span class="hljs-keyword">in</span> reviews_split <span class="hljs-keyword">if</span> review] <span class="hljs-comment"># 去除空字符串</span><br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(reviews_split))<br>all_text = <span class="hljs-string">&#x27; &#x27;</span>.join(reviews_split)<br></code></pre></td></tr></table></figure></li><li><p>去除训练数据中的空格</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># create a list of words  # 去除空格</span><br>words = all_text.split()<br></code></pre></td></tr></table></figure></li></ul><p>​前面两步去除完之后得到的<code>reviews</code>、<code>reviews_split</code>都是分割后形成的列表, 为了方便后面的数据清洗操作, 一般都要再合成字符串, 所以要进行<code>&#39; &#39;.join(reviews_split)</code>这样的操作。</p><p>​合理的运用<strong>列表推导式</strong>会使得你的代码显得简洁易懂。</p><ul><li><p>word2int 映射</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> Counter<br><br><span class="hljs-comment"># 将每个单词映射到一个唯一的整数 ID 下面三行代码是按照单词出现的次序从高到低给单词一个整数值的映射</span><br><span class="hljs-comment"># 比如单词 &#x27;me&#x27; 出现的最多那么它的整数值映射就是1</span><br>counts = Counter(words)<br>vocab = <span class="hljs-built_in">sorted</span>(counts, key=counts.get, reverse=<span class="hljs-literal">True</span>)<br>vocab_to_int = &#123;word: ii <span class="hljs-keyword">for</span> ii, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(vocab, <span class="hljs-number">1</span>)&#125; <span class="hljs-comment"># 这里是从1开始映射的</span><br><br><span class="hljs-comment"># 这是一个二维列表, 每一行存储的是一条review的整数映射</span><br>reviews_ints = []<br><span class="hljs-keyword">for</span> review <span class="hljs-keyword">in</span> reviews_split:<br>    reviews_ints.append([vocab_to_int[word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> review.split()])<br></code></pre></td></tr></table></figure></li><li><p>剪枝与填充</p><p>理想情况下我们输入的训练数据都是等长的, 这样能极大的简化模型的定义, 为了实现训练数据等长, 我们需要</p><p>训练数据进行剪枝与填充, 我们需要规定一个共用的长度, 当训练数据长度大于该标准长度时, 需要进行剪枝, 反之, 则需要进行填充。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">-len(row) 是一个负索引，表示从数组的结尾往前数 len(row) 个位置</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">pad_features</span>(<span class="hljs-params">reviews_ints, seq_length</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27; eg: seq=3 [15,25,20,16,50] -&gt; [20, 16, 50]</span><br><span class="hljs-string">      [9] -&gt; [0,0,9]</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <br>    <span class="hljs-comment"># getting the correct rows x cols shape</span><br>    features = np.zeros((<span class="hljs-built_in">len</span>(reviews_ints), seq_length), dtype=<span class="hljs-built_in">int</span>)<br><br>    <span class="hljs-comment"># for each review, I grab that review and </span><br>    <span class="hljs-keyword">for</span> i, row <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(reviews_ints):<br>        features[i, -<span class="hljs-built_in">len</span>(row):] = np.array(row)[:seq_length]<br>    <br>    <span class="hljs-keyword">return</span> features<br></code></pre></td></tr></table></figure></li><li><p>标签数据的清洗</p><p>因为标签只有两类 negative 和  positive, 因此只要将其映射到 0 和 1 即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">训练标签的数据清洗</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>labels_split = labels.split(<span class="hljs-string">&#x27;\n&#x27;</span>)<br>encoded_labels = [label <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> labels_split <span class="hljs-keyword">if</span> label]<br>encoded_labels = np.array([<span class="hljs-number">1</span> <span class="hljs-keyword">if</span> label == <span class="hljs-string">&#x27;positive&#x27;</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span> <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> encoded_labels])<br></code></pre></td></tr></table></figure></li></ul><h4 id="数据集的划分"><a href="#数据集的划分" class="headerlink" title="数据集的划分"></a>数据集的划分</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">split_frac = <span class="hljs-number">0.8</span><br><br><span class="hljs-comment">## split data into training, validation, and test data (features and labels, x and y)</span><br>split_idx = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">len</span>(features)*split_frac)<br>train_x, remaining_x = features[:split_idx], features[split_idx:]<br>train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]<br><br>test_idx = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">len</span>(remaining_x)*<span class="hljs-number">0.5</span>)<br>val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]<br>val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]<br></code></pre></td></tr></table></figure><h4 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h4><p>​这次任务中模型的定义分成了三部分, 构造函数(搭建模型), 前递函数, 隐藏状态的初始化。</p><ul><li><p>构造函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=<span class="hljs-number">0.5</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Initialize the model by setting up the layers.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>().__init__()<br><br>        <span class="hljs-variable language_">self</span>.output_size = output_size<br>        <span class="hljs-variable language_">self</span>.n_layers = n_layers<br>        <span class="hljs-variable language_">self</span>.hidden_dim = hidden_dim<br>        <br>        <span class="hljs-comment"># embedding and LSTM layers</span><br>        <span class="hljs-variable language_">self</span>.embedding = nn.Embedding(vocab_size, embedding_dim)<br>        <span class="hljs-variable language_">self</span>.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, <br>                            dropout=drop_prob, batch_first=<span class="hljs-literal">True</span>)<br>        <br>        <span class="hljs-comment"># dropout layer</span><br>        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(<span class="hljs-number">0.3</span>)<br>        <br>        <span class="hljs-comment"># linear and sigmoid layers</span><br>        <span class="hljs-variable language_">self</span>.fc = nn.Linear(hidden_dim, output_size)<br>        <span class="hljs-variable language_">self</span>.sig = nn.Sigmoid()<br></code></pre></td></tr></table></figure></li><li><p>前递函数定义</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, hidden</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Perform a forward pass of our model on some input and hidden state.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        batch_size = x.size(<span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># embeddings and lstm_out</span><br>        x = x.long()<br>        embeds = <span class="hljs-variable language_">self</span>.embedding(x)<br>        lstm_out, hidden = <span class="hljs-variable language_">self</span>.lstm(embeds, hidden)<br>        <br>        lstm_out = lstm_out[:, -<span class="hljs-number">1</span>, :] <span class="hljs-comment"># getting the last time step output</span><br>        <br>        <span class="hljs-comment"># dropout and fully-connected layer</span><br>        out = <span class="hljs-variable language_">self</span>.dropout(lstm_out)<br>        out = <span class="hljs-variable language_">self</span>.fc(out)<br>        <span class="hljs-comment"># sigmoid function</span><br>        sig_out = <span class="hljs-variable language_">self</span>.sig(out)<br>        <br>        <span class="hljs-comment"># return last sigmoid output and hidden state</span><br>        <span class="hljs-keyword">return</span> sig_out, hidden<br></code></pre></td></tr></table></figure></li><li><p>隐藏状态的初始化</p><p>这里是同时初始化了 c 和 h, 只不过全放在了一个 h 里面</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">init_hidden</span>(<span class="hljs-params">self, batch_size</span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27; Initializes hidden state &#x27;&#x27;&#x27;</span><br>        <span class="hljs-comment"># Create two new tensors with sizes n_layers x batch_size x hidden_dim,</span><br>        <span class="hljs-comment"># initialized to zero, for hidden state and cell state of LSTM</span><br>        weight = <span class="hljs-built_in">next</span>(<span class="hljs-variable language_">self</span>.parameters()).data<br>        <br>        <span class="hljs-keyword">if</span> (train_on_gpu):<br>            hidden = (weight.new(<span class="hljs-variable language_">self</span>.n_layers, batch_size, <span class="hljs-variable language_">self</span>.hidden_dim).zero_().cuda(),<br>                  weight.new(<span class="hljs-variable language_">self</span>.n_layers, batch_size, <span class="hljs-variable language_">self</span>.hidden_dim).zero_().cuda())<br>        <span class="hljs-keyword">else</span>:<br>            hidden = (weight.new(<span class="hljs-variable language_">self</span>.n_layers, batch_size, <span class="hljs-variable language_">self</span>.hidden_dim).zero_(),<br>                      weight.new(<span class="hljs-variable language_">self</span>.n_layers, batch_size, <span class="hljs-variable language_">self</span>.hidden_dim).zero_())<br>        <br>        <span class="hljs-keyword">return</span> hidden<br></code></pre></td></tr></table></figure></li></ul><h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h4><p>​关于模型训练我想单独写一篇博客来讲一下, 不然在这详细写篇幅有点大。</p><div class="note note-info">            <p>​2024.7.16: 挖个坑, 等自己填, 我感觉应该不会太久, 关于 pytorch 的模型训练流程我想写好久了, 一直没机会写。</p>          </div><h4 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h4><p>​模型评估有一个比较完整的体系, 不过我们这种小项目对于模型的评估其实就是自己编写一些示例来测试一下模型的性能, 看看泛化性怎么样。</p><ul><li><p>定义编码输入的句子函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_review</span>(<span class="hljs-params">test_review</span>):<br>    test_review = test_review.lower() <span class="hljs-comment"># lowercase</span><br>    <span class="hljs-comment"># get rid of punctuation</span><br>    test_text = <span class="hljs-string">&#x27;&#x27;</span>.join([c <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> test_review <span class="hljs-keyword">if</span> c <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> punctuation])<br><br>    <span class="hljs-comment"># splitting by spaces</span><br>    test_words = test_text.split()<br><br>    <span class="hljs-comment"># tokens</span><br>    test_ints = []<br>    test_ints.append([vocab_to_int.get(word, <span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> test_words])<br><br>    <span class="hljs-keyword">return</span> test_ints<br></code></pre></td></tr></table></figure></li><li><p>定义预测函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">net, test_review, sequence_length=<span class="hljs-number">200</span></span>):<br>    <br>    net.<span class="hljs-built_in">eval</span>()<br>    <br>    <span class="hljs-comment"># tokenize review</span><br>    test_ints = tokenize_review(test_review)<br>    <br>    <span class="hljs-comment"># pad tokenized sequence</span><br>    seq_length=sequence_length<br>    features = pad_features(test_ints, seq_length)<br>    <br>    <span class="hljs-comment"># convert to tensor to pass into your model</span><br>    feature_tensor = torch.from_numpy(features)<br>    <br>    batch_size = feature_tensor.size(<span class="hljs-number">0</span>)<br>    <br>    <span class="hljs-comment"># initialize hidden state</span><br>    h = net.init_hidden(batch_size)<br>    <br>    <span class="hljs-keyword">if</span>(train_on_gpu):<br>        feature_tensor = feature_tensor.cuda()<br>    <br>    <span class="hljs-comment"># get the output from the model</span><br>    output, h = net(feature_tensor, h)<br>    <br>    <span class="hljs-comment"># convert output probabilities to predicted class (0 or 1)</span><br>    pred = torch.<span class="hljs-built_in">round</span>(output.squeeze()) <br>    <span class="hljs-comment"># printing output value, before rounding</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Prediction value, pre-rounding: &#123;:.6f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(output.item()))<br>    <br>    <span class="hljs-comment"># print custom response</span><br>    <span class="hljs-keyword">if</span>(pred.item()==<span class="hljs-number">1</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Positive review detected!&quot;</span>)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Negative review detected.&quot;</span>)<br></code></pre></td></tr></table></figure></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>​这个项目是跟着github上一个教程写的, 在完成这个项目的过程中我接触到了很多以前没有考虑过的东西, 比如数据的清洗、剪枝和填充、word2int、word2vec等等。尤其是数据清洗的流程和小细节, 让我受益良多。虽然这个项目挺简单, 但是它确实增强了我对 NLP 任务 pipline 的理解和掌握。</p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>nlp</tag>
      
      <tag>LSTM</tag>
      
      <tag>RNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[DL]LSTM学习</title>
    <link href="/2024/07/13/DL-LSTM%E5%AD%A6%E4%B9%A0/"/>
    <url>/2024/07/13/DL-LSTM%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><h3 id="RNN的缺陷"><a href="#RNN的缺陷" class="headerlink" title="RNN的缺陷"></a>RNN的缺陷</h3><p>​    上一篇文章学习了 RNN 的原理与应用, 但是当人们应用起 RNN 时, 仍然有着很多缺陷, 比如在处理长序列时，尤其是需要捕捉远距离依赖关系的情况下, RNN表现不佳, 并且容易出现梯度消失与爆炸, 梯度消失与爆炸出现的原因推导可以参考<a href="https://blog.csdn.net/mary19831/article/details/129570030">这篇文章</a>。</p><p>​     远距离的依赖关系举个例子：“红烧排骨”出现在文字的开头，当输入到最后字符串是，RNN网络，可能已经忘记了本序列最重要的单词“红烧排骨”，…..的做法与辣子鸡相似，最后就有可能预测出“辣子鸡”。对于这个远距离依赖关系, 吴恩达的课上讲的通俗易懂, 可以看看。</p><h3 id="GRU-vs-LSTM"><a href="#GRU-vs-LSTM" class="headerlink" title="GRU vs LSTM"></a>GRU vs LSTM</h3><p>​     为了解决以上的问题, GRU 和 LSTM就出现了, GRU (Gate Recurrent Unit)译作门控循环单元, LSTM (Long Short-Term Memory) 译作长短时记忆。这里就不详细介绍 GRU 的原理了, 如果将来我回看时发现需要补充 GRU时, 可以参考一下<a href="https://zhuanlan.zhihu.com/p/32481747">这篇文章</a>。</p><p>​     这里比较一下 GRU 和 LSTM。</p><table><thead><tr><th align="center">网络结构</th><th align="center">LSTM</th><th align="center">GRU</th></tr></thead><tbody><tr><td align="center">复杂度</td><td align="center">结构复杂</td><td align="center">结构简单</td></tr><tr><td align="center">参数量</td><td align="center">参数量大</td><td align="center">参数量小</td></tr><tr><td align="center">计算开销</td><td align="center">大</td><td align="center">小</td></tr><tr><td align="center">灵活度</td><td align="center">更灵活</td><td align="center">较差</td></tr></tbody></table><p>在RNN实现的任务中，不考虑计算资源限制的情况下，常将 LSTM 作为默认选项。</p><h3 id="LSTM-原理"><a href="#LSTM-原理" class="headerlink" title="LSTM 原理"></a>LSTM 原理</h3><p>​     需要注意的是, LSTM 并不是一种全新的、颠覆性的架构，它仍然是在 RNN 的基础上发展来的，LSTM的每一个cell 是 RNN 的每一个 cell 的增强版。</p><p>​     LSTM在原来RNN的基础上增加了更新门、遗忘门、输出门。</p><p><img src="/images/post_3/11.png" alt="LSTM原理图"></p><p>​     从图中可以看到, 相比 RNN, LSTM 的输入由2个(a和x)变为了3个(a&#x2F;x&#x2F;c)，图中的 c~^(t)就是原来 RNN的输出a^(t)。但是在 LSTM 中, a与x要经过更新门、遗忘门、输出门三个门再与c^(t-1)作相应的运算才能得到a^t、c^t, a^t经过 softmax 得到输出 y^t。计算过程如下图所示(是从吴恩达老师课上截的图，但是真的易懂)</p><p><img src="/images/post_3/10.png" alt="LSTM计算过程"></p><p>​    从图中可以得到需要学习的参数, 原来 RNN 的 Wa、Ba变成了 Wc、Bc。又新增了 Wu、Bu、Wf、Bf、Wo、Bo这六个权重。</p><div class="note note-info">            <p>​2024.7.13: 渐渐地懂了序列模型的一般结构, 感觉 LSTM 的思想好牛啊, 感觉已经是当时局限思想情况下的一个极致的时间序列模型了</p>          </div><p>​      再结合 BRNN 的结构出现了双向的 LSTM 网络结构, 目前使用 RNN 模型大部分都是双向的 LSTM 结构，有一种集百家之所长的感觉。双向 LSTM 结构就不细讲了，知道了 BRNN 和 LSTM 结构很容易就能理解 LSTM 结构。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>​    从 RNN 到 LSTM, 原来模型的发展从来都不是一蹴而就的, 人们永远无法全面的预测到当前提出的模型所存在的所有缺陷, 模型的发展给我一种问题导向的感觉, CNN 的出现解决了图像分类的问题, 但是时序预测还卡着人们, 接着出现了 RNN, 但是 RNN 的出现暴露了很多缺陷, 继而又出现了 LSTM, LSTM 虽然解决了 RNN的一些问题，但是仍未跳脱出时序顺序的输入, 可能当时的人们也认为时序顺序的输入是解决一些时序预测问题的最佳解决策略了, 后来 transform 的出现又颠覆了这一认知。</p><blockquote><p>​    “那未来又会发生什么呢？”</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>nlp</tag>
      
      <tag>LSTM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[DL]RNN学习</title>
    <link href="/2024/07/13/DL-RNN%E5%AD%A6%E4%B9%A0/"/>
    <url>/2024/07/13/DL-RNN%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="RNN介绍"><a href="#RNN介绍" class="headerlink" title="RNN介绍"></a>RNN介绍</h2><h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>​    RNN(Recurrent Neural Network)一般中文译作循环神经网络，是一种序列模型，常用来处理 nlp 等领域的多种任务，包括但不限于语言生成、机器翻译、语音识别、音乐生成等等。在 nlp 领域的发展起了很重要的作用，也是很多研究者必要掌握的模型。</p><p>​    <img src="/images/post_3/2.png" alt="RNN应用领域"></p><p>​    RNN 与 视觉领域的模型(如CNN)的很大区别在于， RNN 网络是将时序因素考虑在内的，在学习RNN时常见”时间步(time step)”这一概念。举例来说，在训练图像分类模型时，图片输入模型的顺序并无要求，但是在训练序列模型时，训练数据输入的顺序有了必要要求。</p><h3 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a>原理介绍</h3><p>​    下面介绍的模型训练数据默认为 one-hot 编码。</p><h4 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h4><p>​    通常情况下 RNN 架构的网络不会有很多层(一般不大于3层), 下面以一层举例, 每一层的输入是 a^t 和 x^t 这两个变量,  输出是 a^(t+1) 和 y^t ,  y^t 是由 sigmoid 激活函数激活的, 表示此处为各个 word 的概率, a^(t+1) 一般是由 tanh 激活函数激活的, 作为下一时间步网络的输入与 x^(t+1) 一同输入模型, 依此一直循环。a^(t+1) 和 y^t 在图中给出。</p><p>​    <img src="/images/post_3/1.png" alt="RNN前向传播原理"></p><p>​     注意上面给出的原理图只是为了方便理解每个时间步干了什么, 把循环展开了, 真正的 RNN 结构图其实是循环的。</p><p>​    <img src="/images/post_3/14.png" alt="RNN原理图"></p><p>​     从前向传播原理图中可以得出在训练过程中模型学习的参数是 Wa、Wy、Ba、By。</p><h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h4><p>​    了解模型的反向传播可以帮我们更好的理解模型学习的过程, 其实单纯是我把反向传播过程的一些细节给忘了, 想回忆一下。关于反向传播基本概念和原理可以参考一下<a href="https://zhuanlan.zhihu.com/p/261710847">这篇文章</a>。下图是关于 RNN 的反向传播过程。</p><p>​    <img src="/images/post_3/3.png" alt="RNN反向传播过程"></p><p>​     图中的红色箭头表明的反向传播路径。简单来说就是定义一个损失函数(有的文章叫做误差函数)，根据前向传播得到的输出与实际值计算损失函数，再通过链式法则对权重(在这里是Wa\Wy\Ba\By)进行求导。</p><p>​     将原来的权重值 — 求导的结果*lr 得到新的权重完成梯度下降的过程。</p><h3 id="不同输出的-RNN-网络"><a href="#不同输出的-RNN-网络" class="headerlink" title="不同输出的 RNN 网络"></a>不同输出的 RNN 网络</h3><p>​    在RNN应用领域那张图上可以看到 RNN 有很多应用领域, 模型的输入和输出并不一定是严格一对一的，特定的任务情境下 RNN 网络的输出数量并不一定等于输入的数量, 因此要根据特定任务场景调整网络的输出。</p><h4 id="一对多网络"><a href="#一对多网络" class="headerlink" title="一对多网络"></a>一对多网络</h4><p>   一对多常用在生成任务上, 比如音乐的生成, 输入可以为空或者一个表示音乐基调的单词, 输出则为不等长的音符组成音乐，这种网络结构如下图所示。</p><p>​        <img src="/images/post_3/6.png" alt="One2Many"></p><h4 id="多对一网络"><a href="#多对一网络" class="headerlink" title="多对一网络"></a>多对一网络</h4><p>​    多对一网络常用在舆论检测上, 比如饭店评价通过输入一段话来评判客户对这家饭店的打星, 比如打星范围为1-5, 那么模型的唯一输出则为打星值。这种网络结构如下图所示。</p><p>​    <img src="/images/post_3/7.png" alt="Many2One">    </p><h4 id="多对多网络"><a href="#多对多网络" class="headerlink" title="多对多网络"></a>多对多网络</h4><p>​    这种网络最常见, 有时候输入的序列长度跟输出序列长度并不相等, 比如机器翻译应用。这种网络结构如下图所示，我们常将带有 x 输入的模型的前半部分叫做 encoder(编码器), 带有 y 输出的模型后半部分叫做 decoder(译码器)。</p><p>​    <img src="/images/post_3/15.png" alt="Many2Many">        </p><h3 id="双向-RNN-BRNN"><a href="#双向-RNN-BRNN" class="headerlink" title="双向 RNN (BRNN)"></a>双向 RNN (BRNN)</h3><p>   RNN 网络虽然解决了序列输入的训练问题，但是如果碰到类似于下面这种训练输入情况</p><figure class="highlight smalltalk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs smalltalk"><span class="hljs-comment">&quot;Hello, Teddy Bear is on sale.&quot;</span><br><span class="hljs-comment">&quot;Hello, Teddy is coming.&quot;</span><br></code></pre></td></tr></table></figure><p>   RNN 在序列预测时, 如果前面的三个词是 “Hello, Teddy”, 这时候模型很难正确预测下面的输出是什么，这也是单向 RNN 网络的局限性，为了解决这种问题，出现了双向循环网络(Bidirectional RNN), 从左向右与从右向左同时开始训练。原理图如下所示。</p><p>​    <img src="/images/post_3/12.png" alt="BRNN原理图">            </p><h3 id="深层-RNN-DRNN"><a href="#深层-RNN-DRNN" class="headerlink" title="深层 RNN (DRNN)"></a>深层 RNN (DRNN)</h3><p>   在构建 RNN 网络时, 因为时序循环的存在使得网络本身就具有了很多层的网络, 因此 RNN 的层数一般没有很多，但当训练数据量增大时，深层 RNN (Deep RNN) 的效果比单层的 RNN 效果往往更好。深层 RNN 的激活向量的运作如下图所示, 就是单纯地给每层都像以前一样重复操作。</p><p>​    <img src="/images/post_3/13.png" alt="DRNN原理图">     </p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>   RNN的出现极大推动了NLP领域的发展, 最后用科学家们对其评价来结尾。</p><blockquote><p>“RNN和其变种（如LSTM、GRU）在处理时间序列和序列数据方面具有巨大的潜力，尤其是在语音识别、翻译、语音生成等领域。然而，Transformer模型的出现正在改变这一领域的游戏规则，Transformer在处理长序列数据时展示出了更高的效率和性能。”</p><p>​                                                                                                                                                            ——Andrew Ng</p></blockquote><blockquote><p>“尽管RNN在很多应用中表现出色，但在处理长序列时，尤其是需要捕捉远距离依赖关系的情况下，Transformer等新型架构可能会更加适合。”</p><p>​                                                                                                                                                        ——Ilya Sutskever</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>nlp</tag>
      
      <tag>RNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[DL]Self-attention学习</title>
    <link href="/2024/07/10/DL-Self-attention%E5%AD%A6%E4%B9%A0/"/>
    <url>/2024/07/10/DL-Self-attention%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><h4 id="新的任务场景"><a href="#新的任务场景" class="headerlink" title="新的任务场景"></a>新的任务场景</h4><p>​    在前面的分类模型(如CNN等)的学习中, 模型的输入(如图像)是一个固定大小向量且在批输入时的各个输入向量通常并无直接关系, 有些任务下(如文字处理)需要变长的向量作为模型的输入, 且输入的各个向量之间有一定的语义关系，这时就要考虑改进模型以适应这种任务的需求。</p><p>​    通常情况下这种模型的输出有三种情况(如下图所示):    </p><p><img src="/images/post_2/5.png" alt="模型输出的三种情况"></p><ul><li><p>输入的每个向量都对应一个输出 seqence labeling</p><p>常用于词性标注等任务</p></li><li><p>输入的n个变量只对应一个输出</p><p>常用于舆情分析(从一句话中抽取对某事的态度)、药物毒性检测。</p></li><li><p>输入的n个变量对应m个输出 seq2seq</p><p>应用最广泛，机器翻译，分子预测，对话任务等。</p></li></ul><h4 id="新的问题"><a href="#新的问题" class="headerlink" title="新的问题"></a>新的问题</h4><p>​    对 Sequence Labeling 而言，我们想要n个输入的变长向量对应n个输出，我们自然而然的想到可以给每个向量都加上一个 FC 层，这样可以得到n个输出向量，但是这样就完全把语义信息抛弃了，词与词之间的关系没有用上。像 “I saw a saw”这个例子，saw既有动词”看”的意思, 又有名词”锯子”的意思，但是如果你输入同样的 FC 层，输出肯定只能有一种结果。</p><h3 id="Self-Attention-原理"><a href="#Self-Attention-原理" class="headerlink" title="Self-Attention 原理"></a>Self-Attention 原理</h3><h4 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h4><p>​    简单来说， Self-Attention 就是把各个词的相关度也作为输入输进了模型，如下图</p><p><img src="/images/post_2/4.png" alt="将相关度作为输入"></p><p>​     然后再将输出作为输入进入 FC 层(可以再将FC层的输出作为输入再进self-attention层)</p><p><img src="/images/post_2/1.png" alt="Self-Attention的总览"></p><h4 id="实现步骤"><a href="#实现步骤" class="headerlink" title="实现步骤"></a>实现步骤</h4><p>​    目前主流算相关度的方法是Google提出的 Dot-product 方法，以算 a1 与 a2、 a3、 a4 的相关度为例:</p><ul><li><p>先算a1的q(quary, 查找的意思, 去找 a1 与其他输入的相关度) q &#x3D; Wq*a1 (??? 这里的Wq咋来的，是一个标量还是一个向量，算每个输入的 q 时这个Wq是一样的吗)</p></li><li><p>再算其他输入的k, k &#x3D; Wk * an (??? 这里的Wk也同问)，这里也有必要算一下 a1 的 k</p></li><li><p>将a1的q与每个k作点积运算，得到相关系数a1,1 &#x2F; a1,2 等等</p></li><li><p>将得到的相关系数经过一层softmax(不一定非要softmax, ReLU等也可以)得到a’1,1 &#x2F; a’1,2 等等</p><p> <img src="/images/post_2/2.png" alt="求相关度原理图"></p></li><li><p>算每个输入的 v &#x3D; Wv * a1(??? 这里的Wv也同问)</p></li><li><p>将每个 v与对应的 a’ 相乘再相加得到 b1, 如果 b1 很接近 v 与 a’1,2 的结果，那么就跟输入2更相关</p><p> <img src="/images/post_2/3.png" alt="求b的原理图"></p></li></ul><h4 id="矩阵运算"><a href="#矩阵运算" class="headerlink" title="矩阵运算"></a>矩阵运算</h4><ul><li><p>将各个input vector 合成矩阵 I, 李宏毅老师的 ppt 很明了, 竖着放置相当于将各个 vector 竖列组合。再将其分别与Wq、Wk、Wv作矩阵相乘运算, 得到 Q、 K、 V。每列即为算得的qi、ki、vi。</p><p><img src="/images/post_2/6.png" alt="求矩阵QKV"></p></li><li><p>再将 K.T 与 Q 作矩阵乘法, 得到A, 经过softmax得到A’, A’ 的第n行第m列即为 自注意力score a’n,m。</p><p><img src="/images/post_2/7.png" alt="求A&#39;"></p></li><li><p>再将 V 与矩阵 A’ 作矩阵乘法, 得到矩阵 O, 每一列即为 bi。</p><p><img src="/images/post_2/8.png" alt="求A&#39;"></p></li><li><p>上面整个流程如下图所示, 这个流程中所需要学习的参数就是 Wq、Wk、Wv。</p><p><img src="/images/post_2/9.png" alt="self-attention矩阵运算流程"></p></li></ul><h3 id="Self-Attention-vs-CNN"><a href="#Self-Attention-vs-CNN" class="headerlink" title="Self-Attention vs CNN"></a>Self-Attention vs CNN</h3><p>   研究表明, Self-Attention 是一中更加 flexible 的 CNN, 而CNN 则是受限的 Self-Attention。self-attention是可以通过调整某些参数变成CNN的。</p><p><img src="/images/post_2/13.png" alt="Self-Attention vs CNN"></p><h3 id="Muti-head-Self-Attention"><a href="#Muti-head-Self-Attention" class="headerlink" title="Muti-head Self-Attention"></a>Muti-head Self-Attention</h3><p>   关于多头注意力机制，其思路就是将每个q、k、v分成多路，然后照着一样的流程各自去算矩阵O’，最后再将矩阵O’通过运算化成矩阵O。</p><p>   为了方便理解，可以这样想为什么要多头注意力机制: 如果只用一头注意力机制，那么这个q就负责这所有的相关性信息的搜索。如果用多头注意力机制，那么每个q就可以专门负责某一方面的搜寻，使得训练得更全面。</p><p><img src="/images/post_2/10.png" alt="多头注意力机制的原理图"></p><p>​    将q分成多路的运算: [q1,1 , q1,2] &#x3D; W_q_muti x q</p><p>​    K、V分成多路的运算也同理。</p><p>​    这样需要学习的参数又多了 W_q_muti、W_k_muti、W_v_muti、W_o_muti(形状不同前面三个, 作用是将多个O’合成一个O,如图)。</p><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>​    上面提到的 Self-Attention 虽然将各个input vector 之间的关系考虑了进来, 但是仍然忽略了各个input vector 的位置关系。比如训练数据为语言的时候，最前面位置的词一般是主语。这时候就要考虑位置的因素了，这时候可以将每个输入向量加上对应的位置向量ei, 再进行self-attention。</p><p><img src="/images/post_2/11.png" alt="Positional Encoding"></p><p>​    目前有种 ei 的实现方法, transformer 那篇论文中提出的 Postional Encoding 是一种手工实现的 ei, 事先将每个位置的 ei 都规定好了。当然还有将 ei 设置为待学习的参数进行学习的方法。</p><p>​     <a href="https://blog.csdn.net/m0_37605642/article/details/132866365#:~:text=%E6%9C%AC%E8%8A%82%E4%BB%A5%20Sinusoidal">transform中的Postional Encoding</a></p><h3 id="Self-Attention的应用"><a href="#Self-Attention的应用" class="headerlink" title="Self-Attention的应用"></a>Self-Attention的应用</h3><h4 id="Self-Attention-for-Speech"><a href="#Self-Attention-for-Speech" class="headerlink" title="Self-Attention for Speech"></a>Self-Attention for Speech</h4><p>​    语音一般是一个很长的序列, 如果将这个序列中的每个vector的关系都算score的话, 计算量会很大, 所以一般会将计算的范围限制在一定长度。</p><p><img src="/images/post_2/12.png" alt="Self-Attention for Speech"></p><h4 id="Self-Attention-for-Image"><a href="#Self-Attention-for-Image" class="headerlink" title="Self-Attention for Image"></a>Self-Attention for Image</h4><p>   图像也可以作为序列做self-attention。一般输入的图像都是RGB通道的，这时每个 pixel 包含的三个值可以作为sequence的一个vector。这时候sequence的长度取决于图像的大小。</p><p><img src="/images/post_2/13.png" alt="Self-Attention for Image"></p><h4 id="Self-Attention-for-Graph"><a href="#Self-Attention-for-Graph" class="headerlink" title="Self-Attention for Graph"></a>Self-Attention for Graph</h4><p>   图也可以做self-attention, 这时候只需要考虑每个结点与之相连结点的score即可。</p><p><img src="/images/post_2/15.png" alt="Self-Attention for Graph"></p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>nlp</tag>
      
      <tag>注意力机制</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>畸形的内卷</title>
    <link href="/2024/07/09/%E7%95%B8%E5%BD%A2%E7%9A%84%E5%86%85%E5%8D%B7/"/>
    <url>/2024/07/09/%E7%95%B8%E5%BD%A2%E7%9A%84%E5%86%85%E5%8D%B7/</url>
    
    <content type="html"><![CDATA[<h3 id="内卷与方向"><a href="#内卷与方向" class="headerlink" title="内卷与方向"></a>内卷与方向</h3><h4 id="起因"><a href="#起因" class="headerlink" title="起因"></a>起因</h4><p>​今天下午在调飞机闲聊时，刚好聊到了分流的事情，学妹让我给些建议(当然其实我觉得我只能给出每个专业的利弊，最后的定夺是应该结合自己的人生目标选择的)。聊着聊着学弟就聊起了他的舍友：</p><blockquote><p>“我舍友上学期几门课卡了3.9的绩点，最后磨老师全给改成了4.1的绩点。”</p></blockquote><p>​我其实挺看不起这种行为的，大家本来都是一起学的，凭什么你能把绩点给改了，这对别人一点也不公平。</p><p>​其实上面的见怪不怪，但是最让我震惊的是学弟学妹竟然也想这样做，讨论让老师给自己改成绩的方法。我心想我天，这都是什么鬼(先叠一下，假如这篇博客不小心被学弟学妹看到了，不要怪我，在这给你们道个歉)。</p><h4 id="看法"><a href="#看法" class="headerlink" title="看法"></a>看法</h4><p>​聊聊我对这些事的看法吧，先声明是拙见。</p><p>​目前高考之前的教育体制让学生们都变得太”唯分数论”了，他们自信地以为只要我的绩点足够高，那么万事万物都是美好的。可是，然后呢？绩点刷到 4.0x 然后呢？</p><p>​你说，然后我就保研了啊。可是，保研了之后呢？再在研究生阶段接着卷绩点吗？</p><blockquote><p>“你始终都在被绩点、综测推着去做事情，你始终不知道你想要的是什么”</p></blockquote><p>​我发现这种现象越来越严重了，大学慢慢的变成了另一所高中。诚实的说，我在大一的时候也是这样的想法:为什么加基地呢？因为可以加综测(歪一下，但是当时面试的时候我还是说的是因为有很多厉害的学长学姐，可以向他们学习，现在看来太难绷了 hhhhh )。</p><p>​虽然之前我也这样想，但是在基地的这一年多，我慢慢的清楚了自己想要的是什么，我会衡量一下为了实现想要的，我要不要去努力的卷绩点、综测，我应该做些什么去实现我想要的，我应该把绩点、综测搞到什么程度就可以了，毕竟这东西在大学非常耗费时间和经历。</p><p>​这样卷搞得大学生活真的没滋没味，太难受了，至少我的前两年是这样觉得的。</p><h4 id="难以破局"><a href="#难以破局" class="headerlink" title="难以破局"></a>难以破局</h4><p>​其实我觉得只要弄清你要什么，你想得到什么，很容易破局。如果你最终是要去公司工作，这时候你就要考虑一下读研对你的工作到底有没有实质性的帮助。其实我感觉三年的工作经验跟读研三年出来工作很难衡量哪个更好。为什么说难以破局，很多人可能跟我一样，想试试自己有没有科研的潜力，再决定自己未来的道路，这时候就要不得不卷一下或者考研了。哎，很难说。</p><p>​但其实我觉得一个人最终能达到什么样的高度，跟自己的性格高度相关，你如果不知道该咋办了，那就听从自己的内心吧(就是做自己觉得应该做的)，每个人都不一样也很难说。</p><h4 id="结尾"><a href="#结尾" class="headerlink" title="结尾"></a>结尾</h4><p>​原来这里有很多话想说，但是我都删了。</p><p>​想家了。</p><table><thead><tr><th>觉得赵雷的《<a href="https://music.163.com/#/song?id=447926063">朵</a>》很好听 特别是前奏！</th></tr></thead></table>]]></content>
    
    
    <categories>
      
      <category>感悟</category>
      
    </categories>
    
    
    <tags>
      
      <tag>内卷</tag>
      
      <tag>人生目标</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>我的第一篇博客</title>
    <link href="/2024/07/08/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/"/>
    <url>/2024/07/08/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/</url>
    
    <content type="html"><![CDATA[<h3 id="我的第一篇博客"><a href="#我的第一篇博客" class="headerlink" title="我的第一篇博客"></a>我的第一篇博客</h3><h5 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h5><p>​纪念一下自己的第一篇博客。感觉我自己还算是一个喜欢分享的人(maybe)，虽然我一直发私密的动态。暑期在做集训的间隙搭建了这个博客网站，感觉还不错，就当QQ空间&#x2F;朋友圈用了。</p><p>​网站是跟着csdn上的教程搭的，部署在github上，基本上不费工夫，搭搭积木一样。搭好之后对网站进行改造有一种小时候玩4399换装的感觉。</p><h5 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h5><p>​我为什么搭建这个博客？我们从初中就开始学习历史，历史事件的发生一般有三种原因: 根本原因、直接原因、导火索。</p><p>​我觉得我搭这个网站的根本原因就是自己是一个喜欢记录的人，但是又不想让别人看见，所以搭这个博客正好满足了我的需求，不出意外的话，这网站我是不会给朋友们说的。直接原因就是今年无人机视觉太难做了，反复折磨，想着记录一下被折磨的过程。导火索是看到wh学长的博客，很早之前写的博客还有callback，很戳我，我很喜欢这样的记录方式。所以立马就搭了这个博客。</p><h5 id="未来"><a href="#未来" class="headerlink" title="未来"></a>未来</h5><p>​不出意外的话，以后我对生活的一些感悟和经历、一些想法、学习的过程、对技术的理解(虽然也没啥理解)，都会在这里更新，希望我自己能坚持吧。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
